{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0798b87c",
   "metadata": {},
   "source": [
    "# Converter\n",
    "- Quickly and dirtily execute all the commands to process all chunks for BDD100K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3defc09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!aws s3 rm s3://s-laion/ssd-videos/ --recursive --exclude='' --dryrun\n",
    "!aws s3 ls s3://s-laion/ssd-videos/ --human-readable --summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6c7a74",
   "metadata": {},
   "source": [
    "Adjust `truncate_frames` to fully recover the 100M images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997830b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /opt/awesome/data/\n",
    "\n",
    "from subprocess import run, call, PIPE, DEVNULL\n",
    "from tqdm import tqdm\n",
    "from tqdm_logger import TqdmLogger\n",
    "import os, sys\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "logging.basicConfig(stream=sys.stderr, level=logging.INFO)\n",
    "\n",
    "base_path = '/opt/awesome/' #should've `/` in the end\n",
    "base_download_url = \"http://dl.yf.io/bdd100k/video_parts/\" #train/test/val\n",
    "urls = lambda x,y : [\"{}bdd100k_videos_{}_{}.zip\".format(base_download_url, x, str(index).zfill(2)) for index in range(y)]\n",
    "\n",
    "train_urls = urls('train', 70) #70 URLs all to be generated\n",
    "test_urls = urls('test', 20)\n",
    "val_urls = urls('val', 10)\n",
    "final_url_list = train_urls + val_urls #test_urls excluded, no ground truth :/\n",
    "\n",
    "target_url = final_url_list[0]\n",
    "iteration_index = 1\n",
    "\n",
    "def convert_chunk_to_tfrecords(url, it_num):\n",
    "    '''\n",
    "    Converts a video part URL to TFRecords and uploads them to AWS\n",
    "    bucket, ensuring minimal storage usage by deleting redundant copies\n",
    "    '''    \n",
    "    #Downloading video part/chunk\n",
    "    logging.info('--- Download Started ---')\n",
    "    run([\n",
    "        'wget', target_url, '--show-progress'\n",
    "        ], stderr=PIPE, universal_newlines=True)\n",
    "    logging.info('Video Part Download Complete!')\n",
    "\n",
    "    #Deleting useless files\n",
    "    #if os.path.exists('./bdd100k/'):\n",
    "    run([\n",
    "        'rm -rf bdd100k {}mytemp/*'.format(base_path)\n",
    "        ], stderr=PIPE, shell=True)\n",
    "\n",
    "    #Unzipping the data\n",
    "    run([\n",
    "        'unzip {}data/{} && rm *.zip'.format(base_path, target_url.split('/')[-1])\n",
    "        ], stderr=PIPE, shell=True)\n",
    "\n",
    "    #renaming any subdirectory to train and moving info\n",
    "    run([\n",
    "        'mv {}* {}train; mv ./info ./bdd100k/'.format(base_path + \"data/bdd100k/videos/\", base_path + \"data/bdd100k/videos/\",) \n",
    "        ], stderr=PIPE, shell=True)\n",
    "\n",
    "    logging.info('Unzipping and setup completed!')\n",
    "\n",
    "    #Processing - Indexing\n",
    "    run([\n",
    "        'python3', '{}scripts/BDD_Driving_Model/data_prepare/filter.py'.format(base_path), '{}data/bdd100k/'.format(base_path)\n",
    "    ], stderr=PIPE, stdout=DEVNULL)\n",
    "\n",
    "    #Converting to TFrecords - estimated time: 3-5 mins/per video part\n",
    "    run([(\n",
    "        \"mkdir {bp}data/bdd100k/tfrecords; \" #creating dummy folder\n",
    "        \"python3 {bp}scripts/BDD_Driving_Model/data_prepare/prepare_tfrecords.py \"\n",
    "        \"--video_index={bp}data/bdd100k/video_filtered_38_60.txt \"\n",
    "        \"--output_directory={bp}data/bdd100k/tfrecords \"\n",
    "        \"--temp_dir_root={bp}mytemp --num_threads=40 \"\n",
    "        \"--truncate_frames=1200; \"\n",
    "        \"mv {bp}data/bdd100k/info {bp}data/\" #moving info back, keeping only single copy\n",
    "        ).format(bp=base_path)\n",
    "    ], stderr=PIPE, shell=True)\n",
    "\n",
    "    #removing redundant files and clearing trash due to storage constraints\n",
    "    run(['rm','-rf', base_path + 'data/bdd100k/videos'], stderr=PIPE)\n",
    "    run(['rm','-rf', base_path + '.local/share/Trash/*'], stderr=PIPE)\n",
    "\n",
    "    logging.info('=== Video Part Fully Processed ===\\n')\n",
    "\n",
    "    list_of_tfrecord_files = os.listdir(\"/opt/awesome/data/bdd100k/tfrecords/\")\n",
    "    tfrecord_list = ['/opt/awesome/data/bdd100k/tfrecords/'+_ for _ in list_of_tfrecord_files] #s3://s-laion/ssd-videos/tfrecords/train_1/\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(tfrecord_list)\n",
    "    writer = tf.data.experimental.TFRecordWriter('/opt/awesome/data/{}.tfrecord'.format(str(it_num).zfill(6)))\n",
    "    writer.write(dataset) #writing the final TFRecord locally\n",
    "    #because write to AWS is bugged and not fixed with .io.TFRecordWriter\n",
    "\n",
    "    #Moving final TFRecord to AWS\n",
    "    run([\n",
    "        'aws', 's3', 'mv', base_path + 'data/{}.tfrecord'.format(str(it_num).zfill(6)), 's3://s-laion/ssd-videos/'\n",
    "    ], stderr=PIPE)\n",
    "\n",
    "base_path = '/opt/awesome/' #should've `/` in the end\n",
    "\n",
    "log_file = '{}data/tqdm_progress.log'.format(base_path)\n",
    "tqdm_stream = TqdmLogger(log_file)\n",
    "\n",
    "#setup stream for streaming TQDM logs\n",
    "tqdm_stream.reset()\n",
    "\n",
    "#Processing each part of the dataset\n",
    "for it_index, _ in tqdm(enumerate(final_url_list), file = tqdm_stream):\n",
    "    convert_chunk_to_tfrecords(_, it_index)\n",
    "    \n",
    "#Closing up everything and preventing execution of following celss\n",
    "exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2a6fb8",
   "metadata": {},
   "source": [
    "861G\n",
    "- `540` --> `6G`\n",
    "- `1200` --> `14G`\n",
    "- `1080` --> `13G` [__FASTER__]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b38a6ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "729\n",
      "14G\t./sample_tfrecs\n"
     ]
    }
   ],
   "source": [
    "!cd /opt/awesome/data/sample_tfrecs; ls | wc -l\n",
    "!cd /opt/awesome/data/; du -h ./sample_tfrecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2af62e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "from google.protobuf.json_format import MessageToJson\n",
    "\n",
    "path = \"/opt/awesome/data/sample_tfrecs/00a2e3ca-5c856cde.tfrecords\"\n",
    "#path = \"./000001.tfrecord\"\n",
    "\n",
    "raw_dataset = tf.data.TFRecordDataset(path)\n",
    "\n",
    "#inspect a single record\n",
    "for raw_record in raw_dataset.take(1):\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(raw_record.numpy())\n",
    "    #print(example.features.feature['image/height'].int64_list.value)\n",
    "    #print(example.features.feature['image/width'].int64_list.value)\n",
    "    #print(example.features.feature['image/format'].bytes_list.value)\n",
    "    speeds = example.features.feature['image/speeds']\n",
    "    plt_speeds = speeds.float_list.value\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ee419b",
   "metadata": {},
   "source": [
    "```py\n",
    "for d in raw_dataset:\n",
    "    ex = tf.train.Example()\n",
    "    ex.ParseFromString(d.numpy())\n",
    "    m = json.loads(MessageToJson(ex))\n",
    "    break\n",
    "\n",
    "m['features']['feature']['image/speeds']['floatList']['value']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c80e5d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%cd /home/awesome/\n",
    "!mkdir /home/awesome/data/bdd100k/tfrecords\n",
    "!clear; python3 scripts/BDD_Driving_Model/data_prepare/prepare_tfrecords.py --video_index='./data/bdd100k/video_filtered_38_60.txt' --output_directory='/home/awesome/data/bdd100k/tfrecords' --temp_dir_root=/home/awesome/mytemp --num_threads=40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aab41a4",
   "metadata": {},
   "source": [
    "## Sample testing of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df8dbca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the module \"MyDataset\" from /opt/awesome/scripts/BDD_Driving_Model/data_providers/nexar_large_speed.py\n",
    "#I'm currently in /opt/awesome/scripts\n",
    "import sys\n",
    "sys.path.append('/opt/awesome/scripts/BDD_Driving_Model/')\n",
    "from data_providers.nexar_large_speed import MyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1e3f378d",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnrecognizedFlagError",
     "evalue": "Unknown command line flag 'ip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnrecognizedFlagError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-923b79f2b80c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/awesome/scripts/BDD_Driving_Model/data_providers/nexar_large_speed.py\u001b[0m in \u001b[0;36mdata_files\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_files_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'IN DATA_FILES'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;31m# TODO: make sure file name pattern matches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_res\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bdd100k_prepro/lib/python3.5/site-packages/tensorflow/python/platform/flags.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# a flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_parsed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m       \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bdd100k_prepro/lib/python3.5/site-packages/absl/flags/_flagvalues.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, argv, known_only)\u001b[0m\n\u001b[1;32m    668\u001b[0m       \u001b[0msuggestions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_helpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_flag_suggestions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m       raise _exceptions.UnrecognizedFlagError(\n\u001b[0;32m--> 670\u001b[0;31m           name, value, suggestions=suggestions)\n\u001b[0m\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_as_parsed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnrecognizedFlagError\u001b[0m: Unknown command line flag 'ip'"
     ]
    }
   ],
   "source": [
    "data_obj = MyDataset('train')\n",
    "out"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "33a6f3ad06ad2c9ca363766d2f13447ff0360e650580e009491c43b46e355b3f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
